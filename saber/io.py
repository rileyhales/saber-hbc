import glob
import logging
import os
import shutil
from collections.abc import Iterable
from typing import List

import geopandas as gpd
import pandas as pd
import yaml
from natsort import natsorted

logger = logging.getLogger(__name__)

__all__ = [
    'read_config', 'init_workdir', 'get_state', 'get_dir', 'read_table', 'write_table', 'read_gis', 'write_gis',
    'list_cluster_files',

    'COL_MID', 'COL_GID', 'COL_RID', 'COL_CID',
    'COL_STRM_ORD', 'COL_X', 'COL_Y', 'COL_MID_DOWN',
    'COL_RPROP', 'COL_GPROP', 'COL_ASN_MID', 'COL_ASN_GID', 'COL_ASN_REASON',

    'COL_QOBS', 'COL_QMOD', 'COL_QSIM',
    'DIR_TABLES', 'DIR_GIS', 'DIR_CLUSTERS', 'DIR_VALID', 'DIR_LIST',
    'TABLE_ASSIGN',
    'TABLE_CLUSTER_METRICS', 'TABLE_CLUSTER_SSCORES', 'TABLE_CLUSTER_LABELS', 'CLUSTER_COUNT_JSON',
    'TABLE_ASSIGN_BTSTRP', 'TABLE_BTSTRP_METRICS',

    'GENERATED_TABLE_NAMES_MAP', 'VALID_YAML_KEYS', 'VALID_GIS_NAMES',
]

# file paths used in this project which should come from the config file
workdir = ''
cluster_data = ''
drain_table = ''
gauge_table = ''
regulate_table = ''
drain_gis = ''
gauge_gis = ''
gauge_data = ''
hindcast_zarr = ''

# processing options
n_processes = 1

# lists for validating
VALID_YAML_KEYS = {'workdir',
                   'cluster_data',
                   'drain_table',
                   'gauge_table',
                   'regulate_table',
                   'drain_gis',
                   'gauge_gis',
                   'gauge_data',
                   'hindcast_zarr',
                   'n_processes', }

VALID_GIS_NAMES = ['drain_gis', 'gauge_gis']

# assign table and gis_input file required column names
COL_MID = 'model_id'  # model id column name: in drain_table, gauge_table, regulate_table, cluster_table
COL_GID = 'gauge_id'  # gauge id column name: in gauge_table
COL_RID = 'reg_id'  # regulate id column name: in regulate_table
COL_CID = 'clstr_id'  # cluster column name: in cluster_table

COL_STRM_ORD = 'strahler_order'  # strahler order column name: in drain_table
COL_X = 'x'  # x coordinate column name: in drain_table
COL_Y = 'y'  # y coordinate column name: in drain_table
COL_MID_DOWN = 'downstream_model_id'  # downstream model id column name: in drain_table

COL_RPROP = 'rprop'  # regulated stream propagation: created by assign_table
COL_GPROP = 'gprop'  # gauged stream propagation: created by assign_table
COL_ASN_MID = 'asgn_mid'  # assigned model id column name: in assign_table
COL_ASN_GID = 'asgn_gid'  # assigned gauge id column name: in assign_table
COL_ASN_REASON = 'reason'  # reason column name: in assign_table

all_cols = [COL_MID,
            COL_GID,
            COL_RID,
            COL_STRM_ORD,
            COL_X,
            COL_Y,
            COL_RPROP,
            COL_GPROP,
            COL_CID,
            COL_MID_DOWN,
            COL_ASN_MID,
            COL_ASN_GID,
            COL_ASN_REASON, ]

atable_cols = [COL_ASN_MID, COL_ASN_GID, COL_ASN_REASON, COL_RPROP, COL_GPROP]
atable_cols_defaults = ['unassigned', 'unassigned', 'unassigned', '', '']

# discharge dataframe columns names
COL_QOBS = 'Qobs'
COL_QMOD = 'Qmod'
COL_QSIM = 'Qsim'

# required workdir folders
DIR_TABLES = 'tables'
DIR_GIS = 'gis'
DIR_CLUSTERS = 'clusters'
DIR_VALID = 'validation'
DIR_LIST = [DIR_TABLES, DIR_GIS, DIR_CLUSTERS, DIR_VALID]

# name of the required input tables
TABLE_ASSIGN = 'assign_table.parquet'

# tables generated by the clustering functions
TABLE_CLUSTER_METRICS = 'cluster_metrics.csv'
TABLE_CLUSTER_SSCORES = 'cluster_sscores.csv'
TABLE_CLUSTER_LABELS = 'cluster_labels.parquet'
CLUSTER_COUNT_JSON = 'best-fit-cluster-count.json'

# tables produced by the bootstrap validation process
TABLE_ASSIGN_BTSTRP = 'assign_table_bootstrap.csv'
TABLE_BTSTRP_METRICS = 'bootstrap_metrics.csv'

GENERATED_TABLE_NAMES_MAP = {
    'assign_table': TABLE_ASSIGN,
    'assign_table_bootstrap': TABLE_ASSIGN_BTSTRP,
    'bootstrap_metrics': TABLE_BTSTRP_METRICS,
    'cluster_metrics': TABLE_CLUSTER_METRICS,
    'cluster_sscores': TABLE_CLUSTER_SSCORES,
    'cluster_table': TABLE_CLUSTER_LABELS,
}

GIS_BOOTSTRAP = 'bootstrap_gauges.gpkg'

GENERATE_GIS_NAMES_MAP = {
    'bootstrap_gauges': GIS_BOOTSTRAP
}


def read_config(config: str) -> None:
    """
    Read the config file to set paths and values

    Args:
        config: path to the config file

    Returns:
        None
    """
    # open a yml and read to dictionary
    with open(config, 'r') as f:
        config_dict = yaml.safe_load(f)

    if config_dict is None:
        raise ValueError('Config file is empty')

    # set global variables
    for key, value in config_dict.items():
        if key not in VALID_YAML_KEYS:
            logger.error(f'Ignored invalid key in config file: "{key}". Consult docs for valid keys.')
            continue
        logger.info(f'Config: {key} = {value}')
        globals()[key] = value

    # validate inputs
    if not os.path.isdir(workdir):
        logger.warning(f'Workspace directory does not exist: {workdir}')
    if not os.path.exists(drain_gis):
        logger.warning(f'Drainage network GIS file does not exist: {drain_gis}')
    if not os.path.exists(gauge_gis):
        logger.warning(f'Gauge network GIS file does not exist: {gauge_gis}')
    if not os.path.isdir(gauge_data):
        logger.warning(f'Gauge data directory does not exist: {gauge_data}')
    if not glob.glob(hindcast_zarr):
        logger.warning(f'Hindcast zarr directory does not exist or is empty: {hindcast_zarr}')

    return


def init_workdir(path: str = None, overwrite: bool = False) -> None:
    """
    Creates the correct directories for a Saber project within the specified directory

    Args:
        path: the path to a directory where you want to create workdir subdirectories
        overwrite: boolean flag, delete existing directories and files and recreate the directory structure?

    Returns:
        None

    Raises:
        NotADirectoryError: if the path is not a directory
    """
    if path is None:
        path = workdir

    if not os.path.exists(path):
        logger.warning(f'Provided path to workdir does not exist. Attempting to create: {path}')
        os.makedirs(path)
    elif overwrite:
        logger.warning(f'overwrite=True, Deleting existing workdir: {workdir}')
        shutil.rmtree(path)
        os.makedirs(path)

    for d in DIR_LIST:
        p = os.path.join(path, d)
        if not os.path.exists(p):
            os.mkdir(p)
    return


def get_state(prop) -> int or str:
    """
    Get a state variable provided by the config or a controlled global variable

    Args:
        prop: name of the global variable

    Returns:
        value of the global variable
    """
    assert prop in globals(), ValueError(f'"{prop}" is not a recognized project state key')
    return globals()[prop]


def get_dir(dir_name: str) -> str:
    """
    Get the path to a directory within the workspace

    Args:
        dir_name: name of the directory

    Returns:
        path to the directory
    """
    assert dir_name in [DIR_TABLES, DIR_GIS, DIR_CLUSTERS, DIR_VALID], f'"{dir_name}" is not a valid directory name'
    table_path = os.path.join(workdir, dir_name)
    if not os.path.exists(table_path):
        logger.warning(f'"{dir_name}" directory does not exist. Error imminent: {table_path}')
    return table_path


def read_table(table_name: str) -> pd.DataFrame:
    """
    Read a table from the project directory by name.

    Args:
        table_name: name of the table to read

    Returns:
        pd.DataFrame

    Raises:
        FileNotFoundError: if the table does not exist in the correct directory with the correct name
        ValueError: if the table format is not recognized
    """
    table_path = _get_table_path(table_name)
    if not os.path.exists(table_path):
        raise FileNotFoundError(f'Table does not exist: {table_path}')

    table_format = os.path.splitext(table_path)[-1]
    if table_format == '.parquet':
        return pd.read_parquet(table_path, engine='fastparquet')
    elif table_format == '.feather':
        return pd.read_feather(table_path)
    elif table_format == '.csv':
        return pd.read_csv(table_path, dtype=str)
    else:
        raise ValueError(f'Unknown table format: {table_format}')


def write_table(df: pd.DataFrame, name: str) -> None:
    """
    Write a table to the correct location in the project directory

    Args:
        df: the pandas DataFrame to write
        name: the name of the table to write

    Returns:
        None

    Raises:
        ValueError: if the table format is not recognized
    """
    table_path = _get_table_path(name)
    table_format = os.path.splitext(table_path)[-1]
    if table_format == '.parquet':
        return df.to_parquet(table_path)
    elif table_format == '.feather':
        return df.to_feather(table_path)
    elif table_format == '.csv':
        return df.to_csv(table_path, index=False)
    else:
        raise ValueError(f'Unknown table format: {table_format}')


def read_gis(name: str) -> gpd.GeoDataFrame:
    """
    Read a GIS file from the project directory by name.

    Args:
        name: name of the GIS file to read

    Returns:
        gpd.GeoDataFrame

    Raises:
        ValueError: if the GIS format is not recognized
    """
    assert name in VALID_GIS_NAMES or name in GENERATE_GIS_NAMES_MAP, \
        ValueError(f'"{name}" is not a recognized project state key')
    return gpd.read_file(_get_gis_path(name))


def write_gis(gdf: gpd.GeoDataFrame, name: str) -> None:
    """
    Write a GIS file to the correct location in the project directory

    Args:
        gdf: the geopandas GeoDataFrame to write to disc
        name: the name of the GIS file

    Returns:
        None

    Raises:
        ValueError: if the GIS dataset name is not recognized
    """
    assert name in VALID_GIS_NAMES or name in GENERATE_GIS_NAMES_MAP, \
        ValueError(f'"{name}" is not a recognized GIS dataset name')
    gdf.to_file(_get_gis_path(name), driver='GPKG')
    return


def list_cluster_files(n_clusters: int or Iterable = 'all') -> List[str]:
    """
    Find all the kmeans model files in the project directory.

    Args:
        n_clusters: the number of clusters to find models for. If 'all', all models will be returned

    Returns:
        List of paths to the kmeans model files

    Raises:
        TypeError: if n_clusters is not an int, iterable of int, or 'all'
    """
    kmeans_dir = os.path.join(workdir, DIR_CLUSTERS)
    if n_clusters == 'all':
        return natsorted(glob.glob(os.path.join(kmeans_dir, 'kmeans-*.pickle')))
    elif isinstance(n_clusters, int):
        return glob.glob(os.path.join(kmeans_dir, f'kmeans-{n_clusters}.pickle'))
    elif isinstance(n_clusters, Iterable):
        return natsorted([os.path.join(kmeans_dir, f'kmeans-{i}.pickle') for i in n_clusters])
    else:
        raise TypeError('n_clusters should be of type int or an iterable')


def _get_table_path(table_name: str) -> str:
    """
    Get the path to a table in the project directory by name

    Args:
        table_name: name of the table to find a path for

    Returns:
        Path (str) to the table

    Raises:
        ValueError: if the table name is not recognized
    """
    if table_name in VALID_YAML_KEYS:
        return os.path.join(workdir, globals()[table_name])
    elif table_name in GENERATED_TABLE_NAMES_MAP:
        return os.path.join(workdir, DIR_TABLES, GENERATED_TABLE_NAMES_MAP[table_name])
    elif table_name.startswith('cluster_'):
        # cluster_centers_{n_clusters}.parquet - 1 per cluster
        # cluster_sscores_{n_clusters}.parquet - 1 per cluster
        return os.path.join(workdir, DIR_CLUSTERS, f'{table_name}.parquet')
    else:
        raise ValueError(f'Unknown table name: {table_name}')


def _get_gis_path(name: str) -> str:
    if name in VALID_GIS_NAMES:
        return globals()[name]
    elif name in GENERATE_GIS_NAMES_MAP:
        if 'bootstrap' in name:
            dir_path = get_dir('validation')
        else:
            dir_path = get_dir('gis')
        return os.path.join(dir_path, GENERATE_GIS_NAMES_MAP[name])
    else:
        raise ValueError(f'Unknown GIS name: {name}')
